# Audio Transcription Pipeline

A production-ready audio transcription service that converts audio files into text with timestamps. Built with Python, FastAPI, and includes support for multiple audio formats and concurrent processing.

## ğŸ“‹ Table of Contents
- [Features](#features)
- [Architecture](#architecture)
- [Quick Start](#quick-start)
- [API Documentation](#api-documentation)
- [Design Decisions](#design-decisions)
- [System Design Answers](#system-design-answers)
- [Testing](#testing)
- [Deployment](#deployment)

## âœ¨ Features

- **Multi-format Support**: Handles WAV, MP3, M4A, FLAC, OGG, AAC, WMA
- **Automatic Format Conversion**: Transparently converts all formats to WAV
- **Intelligent Chunking**: Two strategies for handling long files
  - Silence-based chunking for natural segmentation
  - Time-based chunking for consistent processing
- **Timestamped Segments**: Each transcribed segment includes start/end times
- **Async Processing**: Non-blocking API with background job processing
- **Job Management**: Track transcription status and retrieve results
- **Error Handling**: Comprehensive validation and retry logic
- **Production Ready**: Docker support, health checks, logging

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Client    â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚ HTTP
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      FastAPI Server         â”‚
â”‚  (Async Request Handler)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Background Task Queue     â”‚
â”‚   (Concurrent Processing)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  TranscriptionService       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ 1. Format Validation   â”‚ â”‚
â”‚  â”‚ 2. Format Conversion   â”‚ â”‚
â”‚  â”‚ 3. Audio Chunking      â”‚ â”‚
â”‚  â”‚ 4. Speech Recognition  â”‚ â”‚
â”‚  â”‚ 5. Result Aggregation  â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     Storage Layer           â”‚
â”‚  - Uploaded Files           â”‚
â”‚  - Transcription Results    â”‚
â”‚  - Job Metadata             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ Quick Start

### Prerequisites
- Python 3.11+
- FFmpeg (for audio format conversion)

### Installation

1. Clone the repository:
```bash
git clone <repository-url>
cd transcription-pipeline
```

2. Install FFmpeg:
```bash
# Ubuntu/Debian
sudo apt-get install ffmpeg

# macOS
brew install ffmpeg

# Windows
# Download from https://ffmpeg.org/download.html
```

3. Install Python dependencies:
```bash
pip install -r requirements.txt
```

4. Run the API server:
```bash
python api.py
# or
uvicorn api:app --reload
```

The API will be available at `http://localhost:8000`

### Docker Deployment

```bash
# Build and run with Docker Compose
docker-compose up --build

# API available at http://localhost:8000
```

## ğŸ“š API Documentation

### Submit Transcription Job
```bash
POST /transcribe
Content-Type: multipart/form-data

# Example with curl
curl -X POST "http://localhost:8000/transcribe" \
  -F "file=@sample_audio.mp3"

# Response
{
  "job_id": "550e8400-e29b-41d4-a716-446655440000",
  "status": "pending",
  "message": "Transcription job queued successfully"
}
```

### Check Job Status
```bash
GET /jobs/{job_id}

# Example
curl "http://localhost:8000/jobs/550e8400-e29b-41d4-a716-446655440000"

# Response
{
  "job_id": "550e8400-e29b-41d4-a716-446655440000",
  "status": "completed",
  "file_name": "sample_audio.mp3",
  "created_at": "2024-01-15T10:30:00",
  "updated_at": "2024-01-15T10:31:23",
  "result": {
    "status": "success",
    "transcript": "Hello world, this is a test...",
    "segments": [
      {
        "segment_id": 0,
        "start_time": 0.0,
        "end_time": 60.5,
        "duration": 60.5,
        "text": "Hello world, this is a test...",
        "confidence": "high"
      }
    ],
    "total_duration": 120.3,
    "num_segments": 2
  }
}
```

### List All Jobs
```bash
GET /jobs?status_filter=completed&limit=10

# Example
curl "http://localhost:8000/jobs?status_filter=completed"
```

### Health Check
```bash
GET /health

# Response
{
  "status": "healthy",
  "timestamp": "2024-01-15T10:30:00",
  "active_jobs": 3,
  "total_jobs": 15
}
```

## ğŸ¯ Design Decisions

### 1. Audio Format Handling
**Decision**: Use `pydub` library with FFmpeg backend

**Rationale**:
- FFmpeg supports virtually all audio formats
- Pydub provides Pythonic API over FFmpeg
- Automatic format detection eliminates manual configuration
- Converts everything to standardized WAV for consistent processing

**Implementation**:
```python
def convert_to_wav(self, audio_path: str) -> str:
    audio = AudioSegment.from_file(str(audio_path))
    audio.export(wav_path, format='wav', 
                parameters=["-ac", "1", "-ar", "16000"])
```

### 2. Long Audio File Handling
**Decision**: Dual chunking strategy with fallback

**Strategy 1 - Silence Detection** (Primary):
- Splits audio at natural pauses
- Produces more coherent transcription segments
- Better for conversational audio

**Strategy 2 - Time-based Chunking** (Fallback):
- Fixed 60-second chunks
- Guaranteed to work on any audio
- Used when silence detection fails

**Rationale**:
- Google Speech Recognition has time limits per request
- Chunking enables parallel processing
- Natural breaks improve transcription quality
- Fallback ensures reliability

**Implementation**:
```python
def chunk_audio_by_silence(self, audio_path: str):
    chunks = split_on_silence(
        audio,
        min_silence_len=500,
        silence_thresh=-40,
        keep_silence=200
    )
    
    # Fallback to time-based if needed
    if not chunks or max_chunk_len > threshold:
        return self.chunk_audio_by_time(audio_path)
```

### 3. Speech Recognition Engine
**Decision**: Google Speech Recognition (with abstraction for alternatives)

**Rationale**:
- Free tier available for testing
- No API key required for basic usage
- Easy to swap with Whisper, AssemblyAI, or others
- Good accuracy for general use cases

**Alternatives Considered**:
- **OpenAI Whisper**: Better accuracy, requires more resources
- **AssemblyAI**: Commercial service, better features
- **AWS Transcribe**: Enterprise-grade, higher cost

### 4. Timestamp Generation
**Decision**: Calculate timestamps from chunk positions

**Implementation**:
```python
segments = []
current_time_ms = 0

for chunk in chunks:
    segment = {
        'start_time': current_time_ms / 1000.0,
        'end_time': (current_time_ms + len(chunk)) / 1000.0,
        'duration': len(chunk) / 1000.0,
        'text': transcribed_text
    }
    current_time_ms += len(chunk)
```

**Rationale**:
- Accurate to millisecond precision
- No dependency on speech recognition API timestamps
- Works consistently across different engines

### 5. API Design
**Decision**: Async job-based architecture with FastAPI

**Rationale**:
- Transcription is CPU/IO intensive
- Prevents API timeouts on long files
- Enables concurrent processing
- Client can poll for results
- Better user experience

**Flow**:
1. Client uploads file â†’ Immediate response with job_id
2. Server processes in background
3. Client polls `/jobs/{job_id}` for status
4. Retrieve results when complete

## ğŸ”§ System Design Answers

### How would you handle concurrent uploads?

**Solution**: Async processing with background task queue

**Implementation**:
```python
@app.post("/transcribe")
async def transcribe_audio(
    background_tasks: BackgroundTasks,
    file: UploadFile
):
    job_id = str(uuid.uuid4())
    
    # Save file asynchronously
    await save_upload_file(file, file_path)
    
    # Queue background task
    background_tasks.add_task(
        process_transcription_job, 
        job_id, 
        file_path
    )
    
    return {"job_id": job_id, "status": "pending"}
```

**Scaling Strategies**:

1. **Current (Development)**:
   - FastAPI BackgroundTasks
   - In-memory job storage
   - Suitable for light load

2. **Production (Recommended)**:
   - **Message Queue**: RabbitMQ or Redis Queue
   - **Worker Pool**: Celery workers (horizontal scaling)
   - **Database**: PostgreSQL for job persistence
   - **Object Storage**: S3 for audio files

```
Client â†’ API â†’ Redis Queue â†’ Multiple Workers
                    â†“
              PostgreSQL (job status)
                    â†“
              S3 (audio files & results)
```

3. **Enterprise Scale**:
   - Kubernetes for container orchestration
   - Auto-scaling based on queue depth
   - Load balancer (NGINX/AWS ALB)
   - CDN for result delivery

**Concurrency Limits**:
```python
# Configuration
MAX_CONCURRENT_JOBS = 10
MAX_QUEUE_SIZE = 100

# Rate limiting
from fastapi_limiter import FastAPILimiter
from fastapi_limiter.depends import RateLimiter

@app.post("/transcribe")
@limiter(times=10, minutes=1)  # 10 uploads per minute
async def transcribe_audio(...):
    ...
```

### How would you store audio and transcripts?

**Development Storage**:
```
transcription-pipeline/
â”œâ”€â”€ uploads/           # Temporary audio files
â”œâ”€â”€ transcriptions/    # JSON results
â””â”€â”€ cache/            # Processed audio cache
```

**Production Storage Architecture**:

1. **Audio Files**:
   - **Storage**: AWS S3 / Google Cloud Storage / Azure Blob
   - **Organization**: 
     ```
     s3://bucket-name/
     â”œâ”€â”€ raw/
     â”‚   â””â”€â”€ {year}/{month}/{day}/{job_id}.{ext}
     â”œâ”€â”€ processed/
     â”‚   â””â”€â”€ {year}/{month}/{day}/{job_id}.wav
     â””â”€â”€ archived/  # Old files for compliance
     ```
   - **Lifecycle**: Delete after 30 days or on completion
   - **Security**: Presigned URLs for temporary access

2. **Transcription Results**:
   - **Primary**: PostgreSQL database
   ```sql
   CREATE TABLE transcription_jobs (
       job_id UUID PRIMARY KEY,
       file_name VARCHAR(255),
       file_hash VARCHAR(64),  -- For deduplication
       status VARCHAR(20),
       created_at TIMESTAMP,
       updated_at TIMESTAMP,
       file_url TEXT,
       result_json JSONB  -- Segments with timestamps
   );
   
   CREATE INDEX idx_file_hash ON transcription_jobs(file_hash);
   CREATE INDEX idx_status ON transcription_jobs(status);
   CREATE INDEX idx_created_at ON transcription_jobs(created_at DESC);
   ```

   - **Search**: Elasticsearch for full-text search
   ```json
   {
     "job_id": "...",
     "transcript": "full text",
     "segments": [...],
     "metadata": {
       "duration": 120.5,
       "language": "en"
     }
   }
   ```

3. **Caching Layer**:
   - **Redis** for:
     - Job status (fast lookups)
     - Rate limiting counters
     - Deduplication cache (file hashes)
   
   ```python
   # Check if file already processed
   file_hash = get_file_hash(file)
   cached_result = redis.get(f"transcript:{file_hash}")
   if cached_result:
       return json.loads(cached_result)
   ```

4. **Data Retention**:
   - Active jobs: 90 days in hot storage
   - Archived: 7 years in cold storage (compliance)
   - Implement cleanup cron job

### How do you retry or recover failed transcriptions?

**Multi-layered Retry Strategy**:

**1. Immediate Retry (Transient Failures)**:
```python
import tenacity

@tenacity.retry(
    retry=tenacity.retry_if_exception_type(
        (sr.RequestError, ConnectionError)
    ),
    wait=tenacity.wait_exponential(min=1, max=10),
    stop=tenacity.stop_after_attempt(3)
)
def transcribe_chunk(self, audio_chunk, chunk_index):
    # Attempt transcription
    ...
```

**2. Job-level Retry (Processing Failures)**:
```python
def process_transcription_job(job_id, file_path, retry_count=0):
    MAX_RETRIES = 3
    
    try:
        result = transcription_service.transcribe(file_path)
        
        if result['status'] == 'success':
            update_job_status(job_id, 'completed', result)
        else:
            raise TranscriptionError(result['error'])
            
    except Exception as e:
        if retry_count < MAX_RETRIES:
            # Exponential backoff
            delay = 2 ** retry_count * 60  # 1min, 2min, 4min
            
            # Requeue with delay
            scheduler.add_job(
                process_transcription_job,
                'date',
                run_date=datetime.now() + timedelta(seconds=delay),
                args=[job_id, file_path, retry_count + 1]
            )
        else:
            # Max retries exceeded
            update_job_status(job_id, 'failed', error=str(e))
```

**3. Dead Letter Queue (Permanent Failures)**:
```python
# Jobs that fail after all retries
if retry_count >= MAX_RETRIES:
    # Move to DLQ for manual investigation
    dlq.push({
        'job_id': job_id,
        'file_path': file_path,
        'error': str(e),
        'failed_at': datetime.now(),
        'retry_count': retry_count
    })
    
    # Alert administrators
    send_alert(f"Job {job_id} moved to DLQ")
```

**4. Checkpoint System (Large Files)**:
```python
# Save progress for each chunk
def transcribe_with_checkpoints(file_path, job_id):
    chunks = chunk_audio(file_path)
    
    for i, chunk in enumerate(chunks):
        # Check if already processed
        checkpoint = get_checkpoint(job_id, i)
        if checkpoint:
            continue
            
        text = transcribe_chunk(chunk)
        
        # Save checkpoint
        save_checkpoint(job_id, i, {
            'chunk_index': i,
            'text': text,
            'timestamp': datetime.now()
        })
    
    # Aggregate all checkpoints
    return aggregate_checkpoints(job_id)
```

**5. Recovery on Restart**:
```python
# On application startup
def recover_incomplete_jobs():
    # Find jobs in 'processing' state
    incomplete_jobs = db.query(
        TranscriptionJob
    ).filter_by(status='processing').all()
    
    for job in incomplete_jobs:
        # Check if checkpoints exist
        if has_checkpoints(job.job_id):
            # Resume from checkpoint
            resume_transcription(job)
        else:
            # Restart from beginning
            requeue_job(job)
```

**Error Classification**:
```python
class TranscriptionError(Exception):
    """Base class for transcription errors"""
    
    def is_retryable(self):
        """Override in subclasses"""
        return False

class NetworkError(TranscriptionError):
    def is_retryable(self):
        return True  # Retry network errors

class InvalidAudioError(TranscriptionError):
    def is_retryable(self):
        return False  # Don't retry invalid audio

class QuotaExceededError(TranscriptionError):
    def is_retryable(self):
        return True  # Retry after delay
```

**Monitoring & Alerting**:
```python
# Track failure rates
metrics = {
    'total_jobs': Counter(),
    'failed_jobs': Counter(),
    'retry_count': Histogram(),
}

# Alert on high failure rate
if failed_jobs / total_jobs > 0.1:  # >10% failure
    alert_ops_team("High transcription failure rate")
```

### How would you expose this as an API?

**Current Implementation**: FastAPI with RESTful endpoints

**Complete API Specification**:

```yaml
openapi: 3.0.0
info:
  title: Transcription API
  version: 1.0.0

paths:
  /transcribe:
    post:
      summary: Submit audio for transcription
      requestBody:
        content:
          multipart/form-data:
            schema:
              type: object
              properties:
                file:
                  type: string
                  format: binary
                options:
                  type: object
                  properties:
                    language: string
                    use_silence_detection: boolean
      responses:
        200:
          description: Job created
          content:
            application/json:
              schema:
                type: object
                properties:
                  job_id: string
                  status: string
  
  /jobs/{job_id}:
    get:
      summary: Get job status and result
      parameters:
        - name: job_id
          in: path
          required: true
      responses:
        200:
          description: Job details
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/TranscriptionJob'
  
  /jobs/{job_id}/cancel:
    post:
      summary: Cancel a pending/processing job
      
  /jobs/{job_id}/retry:
    post:
      summary: Retry a failed job

components:
  schemas:
    TranscriptionJob:
      type: object
      properties:
        job_id: string
        status: string
        file_name: string
        created_at: string
        result:
          type: object
          properties:
            transcript: string
            segments: array
```

**Authentication & Security**:
```python
from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials

security = HTTPBearer()

@app.post("/transcribe")
async def transcribe_audio(
    credentials: HTTPAuthorizationCredentials = Depends(security),
    file: UploadFile = File(...)
):
    # Verify API key
    api_key = credentials.credentials
    if not verify_api_key(api_key):
        raise HTTPException(401, "Invalid API key")
    
    # Check user quota
    if not check_quota(api_key):
        raise HTTPException(429, "Quota exceeded")
```

**WebSocket Support** (Real-time Updates):
```python
from fastapi import WebSocket

@app.websocket("/ws/jobs/{job_id}")
async def websocket_job_status(websocket: WebSocket, job_id: str):
    await websocket.accept()
    
    while True:
        job = get_job_status(job_id)
        await websocket.send_json(job)
        
        if job['status'] in ['completed', 'failed']:
            break
            
        await asyncio.sleep(2)
```

**SDK Examples**:

**Python Client**:
```python
from transcription_client import TranscriptionClient

client = TranscriptionClient(api_key="your_key")

# Upload and wait
result = client.transcribe("audio.mp3", wait=True)
print(result.transcript)

# Async upload
job = client.transcribe("audio.mp3", wait=False)
job.wait()  # Block until complete
```

**JavaScript Client**:
```javascript
const client = new TranscriptionClient('your_api_key');

// Upload
const job = await client.transcribe('audio.mp3');

// Poll for result
const result = await job.waitForCompletion();
console.log(result.transcript);
```

**Rate Limiting**:
```python
from slowapi import Limiter
from slowapi.util import get_remote_address

limiter = Limiter(key_func=get_remote_address)

@app.post("/transcribe")
@limiter.limit("10/minute")  # 10 requests per minute
async def transcribe_audio(...):
    ...
```

## ğŸ§ª Testing

```bash
# Run all tests
pytest test_transcription.py -v

# Run with coverage
pytest --cov=. --cov-report=html

# Test API endpoints
pytest test_transcription.py::test_api_health_check
```

## ğŸš¢ Deployment

### Local Development
```bash
python api.py
```

### Docker
```bash
docker build -t transcription-api .
docker run -p 8000:8000 transcription-api
```

### Docker Compose
```bash
docker-compose up -d
```

### Production Checklist
- [ ] Configure production database
- [ ] Set up Redis for job queue
- [ ] Configure cloud storage (S3)
- [ ] Set up monitoring (Prometheus/Grafana)
- [ ] Configure log aggregation (ELK stack)
- [ ] Set up SSL/TLS certificates
- [ ] Configure backup strategy
- [ ] Set up CI/CD pipeline

## ğŸ“Š Performance Metrics

- **Throughput**: ~5-10 concurrent jobs (single server)
- **Processing Speed**: ~1x real-time (60s audio = 60s processing)
- **Formats Supported**: 7 major audio formats
- **Max File Size**: 500MB (configurable)
- **API Response Time**: <100ms (job submission)

## ğŸ”® Future Enhancements

1. **Multiple Speech Recognition Engines**
   - OpenAI Whisper integration
   - AssemblyAI support
   - Custom model training

2. **Advanced Features**
   - Speaker diarization (who said what)
   - Language detection
   - Custom vocabulary
   - Profanity filtering

3. **Performance**
   - GPU acceleration for Whisper
   - Distributed processing
   - Result caching

4. **User Features**
   - Web UI for file upload
   - Real-time transcription (streaming)
   - Export to SRT, VTT formats

## ğŸ“ License

MIT License

## ğŸ‘¤ Author

Your Name - engr.umarfarooq555@gmail.com
